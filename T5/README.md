# T5: Text-to-Text Transfer Transformer

**ë…¼ë¬¸**: "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" (Raffel et al., 2019)

T5ëŠ” ëª¨ë“  NLP íƒœìŠ¤í¬ë¥¼ "í…ìŠ¤íŠ¸ ì…ë ¥ â†’ í…ìŠ¤íŠ¸ ì¶œë ¥" í˜•ì‹ìœ¼ë¡œ í†µì¼í•œ í˜ì‹ ì ì¸ ì ‘ê·¼ë²•ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´

### 1. Text-to-Text Framework
ëª¨ë“  ì–¸ì–´ íƒœìŠ¤í¬ë¥¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬:

```
# Translation
Input:  "translate English to German: Hello world"
Output: "Hallo Welt"

# Summarization  
Input:  "summarize: [long article text]"
Output: "[summary]"

# Question Answering
Input:  "question: What is the capital? context: Paris is the capital of France."
Output: "Paris"

# Classification
Input:  "cola sentence: This sentence is grammatical."
Output: "acceptable"
```

### 2. í†µì¼ëœ ì•„í‚¤í…ì²˜
- **í•˜ë‚˜ì˜ ëª¨ë¸**ë¡œ ëª¨ë“  NLP íƒœìŠ¤í¬ ì²˜ë¦¬
- **ë™ì¼í•œ loss function** (cross-entropy)
- **ë™ì¼í•œ decoding ë°©ì‹**
- **Multi-task learning** ìì—°ìŠ¤ëŸ½ê²Œ ê°€ëŠ¥

## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜

### ë…¼ë¬¸ì˜ ìˆ˜ì‹ê³¼ êµ¬í˜„ ë§¤í•‘

#### 1. Relative Position Encoding
**ë…¼ë¬¸ ìˆ˜ì‹**: Attentionì— relative position bias ì¶”ê°€
```
A_ij = Q_i Â· K_j + b_{clip(i-j, -k, k)}
```

**êµ¬í˜„**:
```python
class RelativePositionBias(nn.Module):
    def forward(self, query_length: int, key_length: int):
        # ìƒëŒ€ì  ìœ„ì¹˜ ê³„ì‚°: i - j
        relative_position = memory_position - context_position
        
        # Bucketìœ¼ë¡œ ë³€í™˜ (ê°€ê¹Œìš´ ê±°ë¦¬ëŠ” ì„¸ë°€í•˜ê²Œ, ë¨¼ ê±°ë¦¬ëŠ” coarseí•˜ê²Œ)
        relative_position_bucket = self._relative_position_bucket(relative_position)
        
        # Bias ê³„ì‚°
        bias = self.relative_attention_bias(relative_position_bucket)
        return bias
```

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
- ì ˆëŒ€ ìœ„ì¹˜ ëŒ€ì‹  **ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„** í•™ìŠµ
- **ë¡œê·¸ ìŠ¤ì¼€ì¼ bucketing**ìœ¼ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬
- **ì²« ë²ˆì§¸ layerì—ë§Œ** relative bias ì ìš©

#### 2. T5 Layer Normalization
**ë…¼ë¬¸**: RMSNormê³¼ ìœ ì‚¬í•˜ì§€ë§Œ centering ì—†ìŒ
```
LayerNorm(x) = x / sqrt(variance + Îµ) * scale
```

**êµ¬í˜„**:
```python
class T5LayerNorm(nn.Module):
    def forward(self, hidden_states):
        # T5ëŠ” meanì„ ë¹¼ì§€ ì•Šê³  varianceë¡œë§Œ normalize
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states
```

**ì°¨ì´ì **:
- BERT/GPT: `(x - mean) / sqrt(variance + Îµ)`
- T5: `x / sqrt(variance + Îµ)` (mean centering ì—†ìŒ)

#### 3. Multi-Head Attention with Relative Bias
**ë…¼ë¬¸ ìˆ˜ì‹**:
```
Attention(Q, K, V) = softmax((QK^T + bias) / sqrt(d_k))V
```

**êµ¬í˜„**:
```python
def forward(self, query, key, value, position_bias=None):
    # Scaled dot-product attention
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
    
    # Add relative position bias
    if position_bias is not None:
        scores += position_bias
    
    # Softmax and apply to values
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
```

#### 4. Pre-Normalization Structure
**T5 íŠ¹ì§•**: Layer normì´ attention/FFN **ì´ì „**ì— ì ìš©

```python
def forward(self, hidden_states):
    # Pre-norm for self-attention
    norm_hidden_states = self.layer_norm_1(hidden_states)
    attention_output = self.self_attention(norm_hidden_states)
    hidden_states = hidden_states + attention_output  # residual
    
    # Pre-norm for feed-forward  
    norm_hidden_states = self.layer_norm_2(hidden_states)
    ff_output = self.feed_forward(norm_hidden_states)
    hidden_states = hidden_states + ff_output  # residual
```

## ğŸ² Pre-training: Span Corruption

### í•µì‹¬ ì•„ì´ë””ì–´
BERTì˜ masked language modelingì„ **ì—°ì†ëœ span**ìœ¼ë¡œ í™•ì¥

### ì•Œê³ ë¦¬ì¦˜
1. **15% í† í°**ì„ corruption ëŒ€ìƒìœ¼ë¡œ ì„ íƒ
2. **í‰ê·  3 í† í°**ì˜ ì—°ì†ëœ spanìœ¼ë¡œ ê·¸ë£¹í™”
3. ê° spanì„ **sentinel í† í°**ìœ¼ë¡œ ëŒ€ì²´
4. ëª¨ë¸ì´ sentinel ìˆœì„œëŒ€ë¡œ **ì›ë³¸ í† í°ë“¤ì„ ì˜ˆì¸¡**

### ì˜ˆì‹œ
```python
# ì›ë³¸ í…ìŠ¤íŠ¸
text = ["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog"]

# Span corruption ì ìš© (noise_density=0.33, mean_span_length=3)
corrupted = ["The", "<extra_id_0>", "fox", "<extra_id_1>", "the", "lazy", "dog"]
target = ["<extra_id_0>", "quick", "brown", "<extra_id_1>", "jumps", "over", "<extra_id_2>"]

# í•™ìŠµ í˜•íƒœ
input_text = "The <extra_id_0> fox <extra_id_1> the lazy dog"
target_text = "<extra_id_0> quick brown <extra_id_1> jumps over <extra_id_2>"
```

### êµ¬í˜„
```python
class SpanCorruption:
    @staticmethod
    def corrupt_spans(text, noise_density=0.15, mean_noise_span_length=3.0):
        # ë§ˆìŠ¤í‚¹í•  í† í° ìˆ˜ ê³„ì‚°
        num_noise_tokens = int(round(len(text) * noise_density))
        
        # í‰ê·  span ê¸¸ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ span ìˆ˜ ê³„ì‚°  
        num_noise_spans = max(1, round(num_noise_tokens / mean_noise_span_length))
        
        # ì§€ìˆ˜ë¶„í¬ì—ì„œ ê° span ê¸¸ì´ ìƒ˜í”Œë§
        # ëœë¤ ìœ„ì¹˜ì— span ë°°ì¹˜
        # Sentinel í† í°ìœ¼ë¡œ ëŒ€ì²´
        
        return corrupted_tokens, target_tokens
```

## ğŸ“Š ëª¨ë¸ í¬ê¸°ë³„ Configuration

| Model | Parameters | d_model | Layers | Heads | d_ff |
|-------|-----------|---------|--------|-------|------|
| T5-Small | 60M | 512 | 6 | 8 | 2,048 |
| T5-Base | 220M | 768 | 12 | 12 | 3,072 |
| T5-Large | 770M | 1024 | 24 | 16 | 4,096 |
| T5-3B | 3B | 1024 | 24 | 32 | 16,384 |
| T5-11B | 11B | 1024 | 24 | 128 | 65,536 |

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ëª¨ë¸ ìƒì„±
```python
from main import create_t5_model

# ë‹¤ì–‘í•œ í¬ê¸°ì˜ ëª¨ë¸ ìƒì„± ê°€ëŠ¥
model = create_t5_model("base")  # 220M parameters
```

### 2. Text-to-Text í˜•ì‹ìœ¼ë¡œ í•™ìŠµ
```python
# ë²ˆì—­ íƒœìŠ¤í¬
input_text = "translate English to German: Hello world"
target_text = "Hallo Welt"

# ìš”ì•½ íƒœìŠ¤í¬  
input_text = "summarize: [ê¸´ ë¬¸ì„œ ë‚´ìš©]"
target_text = "[ìš”ì•½ëœ ë‚´ìš©]"

# Forward pass
logits, loss = model(input_ids, decoder_input_ids=decoder_input_ids, labels=labels)
```

### 3. ìƒì„± (Generation)
```python
# Greedy decodingìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
generated_ids = model.generate(
    input_ids=input_ids,
    max_length=50,
    do_sample=False  # greedy
)
```

## ğŸ” T5ì˜ í˜ì‹ ì  ê¸°ì—¬

### 1. Unified Framework
- **ëª¨ë“  NLP íƒœìŠ¤í¬**ë¥¼ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬
- **íƒœìŠ¤í¬ë³„ í—¤ë“œ ë¶ˆí•„ìš”** (ëª¨ë‘ text generation)
- **Multi-task learning** ìì—°ìŠ¤ëŸ½ê²Œ ì§€ì›

### 2. Transfer Learningì˜ ì²´ê³„ì  ë¶„ì„
ë…¼ë¬¸ì—ì„œ ì²´ê³„ì ìœ¼ë¡œ ì—°êµ¬í•œ ìš”ì†Œë“¤:
- **Pre-training objectives** (MLM vs span corruption vs autoregressive)
- **Architectures** (encoder-decoder vs decoder-only)
- **Unlabeled datasets** (C4, Web crawl ë“±)
- **Transfer approaches** (fine-tuning vs multi-task)
- **Model sizes** (Smallë¶€í„° 11Bê¹Œì§€)

### 3. C4 Dataset
**Colossal Clean Crawled Corpus**:
- Common Crawl ê¸°ë°˜
- 750GBì˜ í•„í„°ë§ëœ ì˜ì–´ í…ìŠ¤íŠ¸
- Deduplicationê³¼ quality filtering ì ìš©

### 4. Relative Position Encoding
- **ì ˆëŒ€ ìœ„ì¹˜ì˜ í•œê³„** ê·¹ë³µ
- **ìƒëŒ€ì  ê±°ë¦¬**ì— ê¸°ë°˜í•œ attention bias
- **ê¸´ ì‹œí€€ìŠ¤**ì—ì„œë„ íš¨ê³¼ì 

## ğŸ“ˆ ì„±ëŠ¥ ë° ì˜í–¥

### GLUE/SuperGLUE ì„±ëŠ¥
T5-11BëŠ” ë‹¹ì‹œ **SOTA** ë‹¬ì„±:
- GLUE: 90.3ì 
- SuperGLUE: 89.3ì 

### í›„ì† ëª¨ë¸ë“¤ì— ë¯¸ì¹œ ì˜í–¥
1. **PaLM, PaLM-2**: Text-to-text paradigm ê³„ìŠ¹
2. **UL2**: T5ì˜ span corruptionì„ ë”ìš± ë°œì „
3. **mT5**: Multilingual í™•ì¥
4. **ByT5**: Byte-level tokenization

## ğŸ¯ í•µì‹¬ êµí›ˆ

### 1. "Everything is Text-to-Text"
NLPì˜ ëª¨ë“  ë¬¸ì œë¥¼ **text generation**ìœ¼ë¡œ í†µì¼í•  ìˆ˜ ìˆë‹¤ëŠ” í†µì°°

### 2. Scale + Transfer Learning
**í° ëª¨ë¸** + **ì¢‹ì€ pre-training** + **íš¨ê³¼ì ì¸ transfer**ì˜ ì¡°í•©

### 3. Systematic Evaluation
ë‹¨ìˆœíˆ ì¢‹ì€ ê²°ê³¼ê°€ ì•„ë‹Œ, **ê° êµ¬ì„±ìš”ì†Œì˜ ê¸°ì—¬ë„**ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„

### 4. Simplicity is Power
ë³µì¡í•œ task-specific êµ¬ì¡° ëŒ€ì‹  **ë‹¨ìˆœí•˜ê³  í†µì¼ëœ ì ‘ê·¼ë²•**ì˜ íš¨ê³¼

## ğŸ’¡ êµ¬í˜„ì˜ í•µì‹¬ í¬ì¸íŠ¸

1. **Relative Position Bias**: ì²« ë²ˆì§¸ layerì—ë§Œ ì ìš©, ë‚˜ë¨¸ì§€ëŠ” ì¬ì‚¬ìš©
2. **T5LayerNorm**: Mean centering ì—†ì´ varianceë§Œìœ¼ë¡œ ì •ê·œí™”
3. **Pre-normalization**: Layer normì´ attention/FFN ì´ì „ì— ì ìš©
4. **Shared Embeddings**: Inputê³¼ output embedding ê³µìœ 
5. **Span Corruption**: ì—°ì†ëœ í† í°ë“¤ì„ sentinelë¡œ ëŒ€ì²´í•˜ëŠ” pre-training

T5ëŠ” **Transfer Learningì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„**ì„ ì œì‹œí•˜ë©°, í˜„ì¬ê¹Œì§€ë„ ë§ì€ ëª¨ë¸ë“¤ì˜ ê¸°ë°˜ì´ ë˜ê³  ìˆëŠ” ì¤‘ìš”í•œ ì—°êµ¬ì…ë‹ˆë‹¤.