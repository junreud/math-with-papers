# UL2: "Unifying Language Learning Paradigms" êµ¬í˜„

ì´ í”„ë¡œì íŠ¸ëŠ” Googleì˜ **"UL2: Unifying Language Learning Paradigms"** ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ í†µí•© ì–¸ì–´ í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ ìˆ˜ì‹ê³¼ ê°œë…ì— ì¶©ì‹¤í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ ë…¼ë¬¸ ê°œìš”

**ë…¼ë¬¸**: "UL2: Unifying Language Learning Paradigms"  
**ì €ì**: Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler  
**í•µì‹¬ ê¸°ì—¬**: ì„œë¡œ ë‹¤ë¥¸ ì–¸ì–´ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ë“¤ì„ í•˜ë‚˜ì˜ í†µí•©ëœ í”„ë ˆì„ì›Œí¬ë¡œ ê²°í•©

## ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´: Mixture of Denoisers (MoD)

### ì „ì²´ êµ¬ì¡°
```
ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ì…ë ¥
         â†“
   Mode Token ì¶”ê°€ (<R>, <S>, <X>)
         â†“
  í•´ë‹¹ ëª¨ë“œë³„ Corruption ì ìš©
         â†“
   Decoder-only Transformer
         â†“
    Target Text ìƒì„±
```

## ğŸ”¬ ì„¸ ê°€ì§€ Denoising íŒ¨ëŸ¬ë‹¤ì„

### 1. R-Denoiser (Regular Span Corruption)

**BERT-style í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„**

**ë…¼ë¬¸ ì„¤ì •**:
- Corruption rate: 15%
- í‰ê·  span length: 3 í† í°
- ì—°ì†ëœ spanë“¤ì„ sentinel í† í°ìœ¼ë¡œ ëŒ€ì²´

**ì½”ë“œ êµ¬í˜„**:
```python
def r_denoiser_corruption(text, corruption_rate=0.15, mean_span_length=3.0):
    # Poisson distributionìœ¼ë¡œ span ê¸¸ì´ ìƒ˜í”Œë§
    span_length = max(1, int(random.expovariate(1.0 / mean_span_length)))
    
    # Sentinel í† í°ìœ¼ë¡œ ëŒ€ì²´
    input_tokens.append(f"<extra_id_{sentinel_id}>")
    target_tokens.append(f"<extra_id_{sentinel_id}>")
    target_tokens.extend(original_span)
```

**ì˜ˆì‹œ**:
```
ì›ë³¸: "The quick brown fox jumps over the lazy dog"
ì…ë ¥: "<R> The quick <extra_id_0> jumps over <extra_id_1> dog"
íƒ€ê²Ÿ: "<extra_id_0> brown fox <extra_id_1> the lazy </s>"
```

### 2. S-Denoiser (Sequential Denoising)

**GPT-style í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„**

**ë…¼ë¬¸ ì„¤ì •**:
- Prefix ê¸¸ì´: ì „ì²´ì˜ 50-90%
- Auto-regressive ë°©ì‹ìœ¼ë¡œ ë‚˜ë¨¸ì§€ ìƒì„±
- Language modeling objective

**ì½”ë“œ êµ¬í˜„**:
```python
def s_denoiser_corruption(text, prefix_length=None):
    if prefix_length is None:
        prefix_length = random.randint(
            int(len(text) * 0.5), 
            int(len(text) * 0.9)
        )
    
    input_tokens = text[:prefix_length]
    target_tokens = text[prefix_length:] + ["</s>"]
```

**ì˜ˆì‹œ**:
```
ì›ë³¸: "The quick brown fox jumps over the lazy dog"
ì…ë ¥: "<S> The quick brown fox"
íƒ€ê²Ÿ: "jumps over the lazy dog </s>"
```

### 3. X-Denoiser (Extreme Denoising)

**ê·¹ë‹¨ì ì¸ denoising ì‘ì—…**

**ë…¼ë¬¸ ì„¤ì •**:
- Corruption rate: 50% (ë§¤ìš° ë†’ìŒ)
- í‰ê·  span length: 32 í† í° (ë§¤ìš° ê¸¸ìŒ)
- ë” challengingí•œ reconstruction ì‘ì—…

**ì½”ë“œ êµ¬í˜„**:
```python
def x_denoiser_corruption(text, corruption_rate=0.5, mean_span_length=32.0):
    # R-denoiserì™€ ê°™ì€ ë°©ì‹ì´ì§€ë§Œ ë” aggressiveí•œ íŒŒë¼ë¯¸í„°
    return r_denoiser_corruption(text, corruption_rate, mean_span_length)
```

**ì˜ˆì‹œ**:
```
ì›ë³¸: "The quick brown fox jumps over the lazy dog and runs fast"
ì…ë ¥: "<X> The <extra_id_0> and runs fast"
íƒ€ê²Ÿ: "<extra_id_0> quick brown fox jumps over the lazy dog </s>"
```

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

### Decoder-only Transformer (PaLM ìŠ¤íƒ€ì¼)

```python
class TransformerBlock(nn.Module):
    def forward(self, x, attention_mask=None):
        # Pre-normalization structure
        norm_x = self.norm1(x)
        attn_output = self.attention(norm_x, attention_mask)
        x = x + self.dropout(attn_output)  # Residual connection
        
        norm_x = self.norm2(x)
        ff_output = self.feed_forward(norm_x)
        x = x + self.dropout(ff_output)   # Residual connection
        
        return x
```

### RMSNorm (Root Mean Square Normalization)

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
RMSNorm(x) = x / RMS(x) * Î³
where RMS(x) = âˆš(mean(xÂ²) + Îµ)
```

**ì½”ë“œ êµ¬í˜„**:
```python
class RMSNorm(nn.Module):
    def forward(self, x):
        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)
        normalized = x / rms * self.weight
        return normalized
```

**LayerNormê³¼ì˜ ì°¨ì´**:
- LayerNorm: `(x - Î¼) / Ïƒ * Î³ + Î²` (í‰ê· ê³¼ ë¶„ì‚° ì‚¬ìš©)
- RMSNorm: `x / RMS(x) * Î³` (í‰ê·  ì œê±°, bias ì—†ìŒ)

### SwiGLU Activation

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
SwiGLU(x) = Swish(xW_gate) âŠ™ (xW_up)
where Swish(x) = x * Ïƒ(x)
```

**ì½”ë“œ êµ¬í˜„**:
```python
class FeedForward(nn.Module):
    def forward(self, x):
        gate = F.silu(self.W_gate(x))  # SiLU = Swish
        up = self.W_up(x)
        hidden = gate * up  # Element-wise multiplication
        output = self.W_down(hidden)
        return output
```

## ğŸ² Mode Switchingê³¼ Special Tokens

### Mode Tokens
```python
class SpecialTokens:
    R_MODE = "<R>"      # R-Denoiser mode
    S_MODE = "<S>"      # S-Denoiser mode  
    X_MODE = "<X>"      # X-Denoiser mode
```

### Sentinel Tokens
```python
SENTINEL_0 = "<extra_id_0>"
SENTINEL_1 = "<extra_id_1>"
# ... ìµœëŒ€ 100ê°œê¹Œì§€
```

**ì‚¬ìš© ë°©ì‹**:
1. **Mode token**ì„ ì…ë ¥ ì‹œí€€ìŠ¤ ë§¨ ì•ì— ì¶”ê°€
2. **Sentinel token**ìœ¼ë¡œ corruptionëœ span í‘œì‹œ
3. ëª¨ë¸ì´ modeì— ë”°ë¼ ë‹¤ë¥¸ í•™ìŠµ ëª©í‘œ ìˆ˜í–‰

## ğŸ“Š í›ˆë ¨ ë¹„ìœ¨ (ë…¼ë¬¸ì—ì„œ ì œì•ˆ)

```python
denoiser_ratios = {
    DenoisingMode.R_DENOISER: 0.25,  # 25%
    DenoisingMode.S_DENOISER: 0.25,  # 25% 
    DenoisingMode.X_DENOISER: 0.50   # 50%
}
```

**X-Denoiserê°€ 50%ì¸ ì´ìœ **:
- ê°€ì¥ challengingí•œ ì‘ì—…
- ëª¨ë¸ì˜ robustí•œ ì´í•´ ëŠ¥ë ¥ í–¥ìƒ
- ë‹¤ì–‘í•œ downstream taskì— ë” ì˜ ì¼ë°˜í™”

## ğŸ”„ í›ˆë ¨ ê³¼ì •

### 1. ë°ì´í„° ì¤€ë¹„
```python
def prepare_training_data(self, texts):
    for text in texts:
        # 1. ëœë¤í•˜ê²Œ denoising ëª¨ë“œ ì„ íƒ
        mode = self.sample_denoising_mode()
        
        # 2. í•´ë‹¹ ëª¨ë“œì— ë”°ë¼ corruption ì ìš©
        if mode == DenoisingMode.R_DENOISER:
            input_tokens, target_tokens = self.r_denoiser_corruption(tokens)
            mode_token = "<R>"
        # ... S, X ëª¨ë“œë„ ë™ì¼
        
        # 3. Mode tokenì„ ë§¨ ì•ì— ì¶”ê°€
        input_tokens = [mode_token] + input_tokens
```

### 2. Loss ê³„ì‚°
```python
def forward(self, input_ids, labels=None):
    # Decoder-onlyì´ë¯€ë¡œ next token prediction
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    
    loss = F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
        ignore_index=-100
    )
```

## ğŸŒŸ UL2ì˜ í˜ì‹ ì„±

### 1. **íŒ¨ëŸ¬ë‹¤ì„ í†µí•©**
- **BERT**: Bidirectional context (R-Denoiserë¡œ ê·¼ì‚¬)
- **GPT**: Auto-regressive generation (S-Denoiser)
- **T5**: Span corruption (R, X-Denoiser)

### 2. **Mode-aware Training**
- í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì—¬ëŸ¬ í•™ìŠµ ëª©í‘œ ë™ì‹œ ìˆ˜í–‰
- Inferenceì‹œ mode tokenìœ¼ë¡œ ì›í•˜ëŠ” ë™ì‘ ì§€ì •
- Multi-task learningì˜ íš¨ê³¼

### 3. **Scalability**
- Decoder-only êµ¬ì¡°ë¡œ scaling ìš©ì´
- PaLM ìŠ¤íƒ€ì¼ì˜ ìµœì í™”ëœ ì•„í‚¤í…ì²˜
- ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œ ê²€ì¦ë¨

## ğŸ¯ Downstream Tasks ì ìš©

### Text Generation
```python
# S-modeë¡œ text generation
input_text = "<S> Once upon a time"
# ëª¨ë¸ì´ auto-regressiveí•˜ê²Œ ì´ì–´ì„œ ìƒì„±
```

### Text Infilling
```python
# R-modeë¡œ text infilling  
input_text = "<R> The weather is <extra_id_0> today"
# ëª¨ë¸ì´ ë¹ˆ ìë¦¬ë¥¼ ì±„ì›Œì„œ ìƒì„±
```

### Summarization
```python
# X-modeë¡œ ê·¹ë‹¨ì ì¸ compression
input_text = "<X> [ê¸´ ë¬¸ì„œ] <extra_id_0>"
# ëª¨ë¸ì´ ì••ì¶•ëœ ìš”ì•½ì„ ìƒì„±
```

## ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ ìš”ì¸

### 1. **Diverse Training Objectives**
```
R-Denoiser: ì–‘ë°©í–¥ ì»¨í…ìŠ¤íŠ¸ ì´í•´
S-Denoiser: ìˆœì°¨ì  ìƒì„± ëŠ¥ë ¥
X-Denoiser: ê·¹ë‹¨ì  ì¶”ìƒí™” ëŠ¥ë ¥
```

### 2. **Architectural Improvements**
```
RMSNorm: ë” ì•ˆì •ì ì¸ í•™ìŠµ
SwiGLU: ë” ì¢‹ì€ í‘œí˜„ ëŠ¥ë ¥
Pre-norm: Gradient flow ê°œì„ 
```

### 3. **Unified Framework**
```
í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ ìˆ˜í–‰
â†’ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„± í–¥ìƒ
â†’ Transfer learning íš¨ê³¼ ê·¹ëŒ€í™”
```

## ğŸ”— ë‹¤ë¥¸ ëª¨ë¸ê³¼ì˜ ë¹„êµ

| ëª¨ë¸ | ì•„í‚¤í…ì²˜ | í•™ìŠµ ëª©í‘œ | íŠ¹ì§• |
|------|----------|-----------|------|
| **BERT** | Encoder-only | MLM, NSP | ì–‘ë°©í–¥ ì´í•´ |
| **GPT** | Decoder-only | Auto-regressive LM | ìƒì„± ëŠ¥ë ¥ |
| **T5** | Encoder-Decoder | Span corruption | Text-to-text |
| **UL2** | Decoder-only | MoD (R+S+X) | **í†µí•© íŒ¨ëŸ¬ë‹¤ì„** |

## ğŸš€ ëª¨ë¸ ì‚¬ìš©ë²•

```python
# ëª¨ë¸ ìƒì„±
model = create_ul2_model("base")

# í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
trainer = UL2Trainer(model, tokenizer)
training_data = trainer.prepare_training_data(texts)

# í›ˆë ¨
for input_ids, labels, mode in training_data:
    loss = trainer.train_step(input_ids, labels)

# ì¶”ë¡  (ê° ëª¨ë“œë³„)
# R-mode: "<R> The weather is <extra_id_0> today"
# S-mode: "<S> Once upon a time"  
# X-mode: "<X> [document] <extra_id_0>"
```

## ğŸ“š ë…¼ë¬¸ì˜ ì‹¤í—˜ ê²°ê³¼

### SuperGLUE Benchmark
- **UL2-20B**: 89.7ì  (ë‹¹ì‹œ SOTA)
- ê¸°ì¡´ T5-11B ëŒ€ë¹„ ìƒë‹¹í•œ ì„±ëŠ¥ í–¥ìƒ

### ì¼ë°˜í™” ëŠ¥ë ¥
- Few-shot learningì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥
- ë‹¤ì–‘í•œ NLP íƒœìŠ¤í¬ì—ì„œ ì¼ê´€ëœ í–¥ìƒ
- íŠ¹íˆ reasoning íƒœìŠ¤í¬ì—ì„œ í° ê°œì„ 

ì´ êµ¬í˜„ì€ UL2 ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´ì¸ **Mixture of Denoisers**ë¥¼ ì¶©ì‹¤íˆ ì¬í˜„í•˜ì—¬, ë‹¤ì–‘í•œ ì–¸ì–´ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ì´ ì–´ë–»ê²Œ í•˜ë‚˜ì˜ í†µí•©ëœ í”„ë ˆì„ì›Œí¬ë¡œ ê²°í•©ë  ìˆ˜ ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.