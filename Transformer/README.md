# Transformer: "Attention Is All You Need" êµ¬í˜„

ì´ í”„ë¡œì íŠ¸ëŠ” Vaswani et al. (2017)ì˜ ë…¼ë¬¸ **"Attention Is All You Need"**ì—ì„œ ì œì•ˆëœ Transformer ì•„í‚¤í…ì²˜ë¥¼ ìˆ˜ì‹ê³¼ ê°œë…ì— ì¶©ì‹¤í•˜ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.

## ğŸ“‹ ë…¼ë¬¸ ê°œìš”

**ë…¼ë¬¸**: "Attention Is All You Need" (NIPS 2017)  
**ì €ì**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, Illia Polosukhin  
**í•µì‹¬ ê¸°ì—¬**: RNNê³¼ CNN ì—†ì´ ìˆœì „íˆ attention ë©”ì»¤ë‹ˆì¦˜ë§Œìœ¼ë¡œ êµ¬ì„±ëœ Transformer ì•„í‚¤í…ì²˜ ì œì•ˆ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ êµ¬ì¡°

### ì „ì²´ êµ¬ì¡°: Encoder-Decoder
```
Input Embeddings + Positional Encoding
         â†“
    Encoder Stack (6 layers)
         â†“
    Decoder Stack (6 layers)
         â†“
    Linear + Softmax
         â†“
    Output Probabilities
```

## ğŸ”¬ í•µì‹¬ ìˆ˜ì‹ê³¼ êµ¬í˜„

### 1. Scaled Dot-Product Attention

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V
```

**ì½”ë“œ êµ¬í˜„**:
```python
def scaled_dot_product_attention(self, Q, K, V, mask=None):
    # QK^T / âˆšd_k
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
    
    if mask is not None:
        scores = scores.masked_fill(mask == 0, float('-inf'))
    
    # softmax(QK^T / âˆšd_k)
    attention_weights = F.softmax(scores, dim=-1)
    
    # softmax(...)V
    output = torch.matmul(attention_weights, V)
    return output, attention_weights
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
- **Q (Query)**: "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€"
- **K (Key)**: "ë¬´ì—‡ê³¼ ë§¤ì¹­í•  ê²ƒì¸ê°€"
- **V (Value)**: "ì‹¤ì œ ì •ë³´"
- **âˆšd_kë¡œ ìŠ¤ì¼€ì¼ë§**: gradient vanishing ë°©ì§€

### 2. Multi-Head Attention

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

**ì½”ë“œ êµ¬í˜„**:
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        self.d_k = d_model // num_heads  # ë…¼ë¬¸ì˜ d_k = d_model / h
        
        # ë…¼ë¬¸ì˜ W^Q_i, W^K_i, W^V_i for all heads
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        
        # ë…¼ë¬¸ì˜ W^O
        self.W_o = nn.Linear(d_model, d_model)
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
- **ë³‘ë ¬ attention**: ì„œë¡œ ë‹¤ë¥¸ representation subspaceì—ì„œ ë™ì‹œì— attention ìˆ˜í–‰
- **h=8 heads**: ë…¼ë¬¸ì—ì„œ 8ê°œì˜ head ì‚¬ìš©
- **d_k = d_v = 64**: d_model=512ë¥¼ 8ê°œ headë¡œ ë‚˜ëˆ”

### 3. Positional Encoding

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**ì½”ë“œ êµ¬í˜„**:
```python
def __init__(self, d_model, max_seq_len=5000):
    pe = torch.zeros(max_seq_len, d_model)
    position = torch.arange(0, max_seq_len).unsqueeze(1)
    
    # ë…¼ë¬¸ ìˆ˜ì‹: 10000^(2i/d_model)
    div_term = torch.exp(torch.arange(0, d_model, 2) * 
                        (-math.log(10000.0) / d_model))
    
    # ë…¼ë¬¸ ìˆ˜ì‹ ì ìš©
    pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜ ì¸ë±ìŠ¤
    pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜ ì¸ë±ìŠ¤
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
- **ìˆœì„œ ì •ë³´ ì œê³µ**: Self-attentionì€ ìˆœì„œë¥¼ ëª¨ë¥´ë¯€ë¡œ ìœ„ì¹˜ ì •ë³´ í•„ìš”
- **Sin/Cos í•¨ìˆ˜**: ìƒëŒ€ì  ìœ„ì¹˜ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŒ
- **ê³ ì • íŒ¨í„´**: í•™ìŠµì´ ì•„ë‹Œ ìˆ˜í•™ì  í•¨ìˆ˜ë¡œ ìƒì„±

### 4. Position-wise Feed-Forward Networks

**ë…¼ë¬¸ ìˆ˜ì‹**:
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```

**ì½”ë“œ êµ¬í˜„**:
```python
class PositionwiseFeedForward(nn.Module):
    def forward(self, x):
        # max(0, xW_1 + b_1) - ReLU activation
        hidden = F.relu(self.W_1(x))
        
        # max(0, xW_1 + b_1)W_2 + b_2
        output = self.W_2(hidden)
        return output
```

**í•µì‹¬ ì•„ì´ë””ì–´**:
- **ê° ìœ„ì¹˜ë³„ë¡œ ë™ì¼í•œ ì—°ì‚°**: ëª¨ë“  positionì— ê°™ì€ FFN ì ìš©
- **ë‘ ë²ˆì˜ ì„ í˜• ë³€í™˜**: 512 â†’ 2048 â†’ 512
- **ReLU í™œì„±í™”**: ë¹„ì„ í˜•ì„± ì œê³µ

## ğŸ”„ Layer êµ¬ì¡°ì™€ Residual Connection

### Encoder Layer
```
x â†’ Multi-Head Self-Attention â†’ Add & Norm â†’ FFN â†’ Add & Norm â†’ output
```

### Decoder Layer
```
x â†’ Masked Multi-Head Self-Attention â†’ Add & Norm
  â†’ Multi-Head Cross-Attention â†’ Add & Norm  
  â†’ FFN â†’ Add & Norm â†’ output
```

**ì½”ë“œ êµ¬í˜„**:
```python
# Encoder Layer
attn_output = self.self_attn(x, x, x, src_mask)
x = self.norm1(x + self.dropout(attn_output))  # Add & Norm

ff_output = self.feed_forward(x)
x = self.norm2(x + self.dropout(ff_output))    # Add & Norm

# Decoder Layer (ì¶”ê°€ë¡œ Cross-Attention)
cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)
x = self.norm2(x + self.dropout(cross_attn_output))  # Add & Norm
```

## ğŸ¯ Attentionì˜ ì¢…ë¥˜

### 1. Encoder Self-Attention
- **ì…ë ¥**: ë™ì¼í•œ ì†ŒìŠ¤ ì‹œí€€ìŠ¤
- **íŠ¹ì§•**: ì–‘ë°©í–¥ìœ¼ë¡œ ëª¨ë“  ìœ„ì¹˜ë¥¼ ë³¼ ìˆ˜ ìˆìŒ
- **ìš©ë„**: ì…ë ¥ ë¬¸ì¥ì˜ ë‚´ë¶€ ê´€ê³„ íŒŒì•…

### 2. Decoder Masked Self-Attention
- **ì…ë ¥**: íƒ€ê²Ÿ ì‹œí€€ìŠ¤ (causal mask ì ìš©)
- **íŠ¹ì§•**: í˜„ì¬ ìœ„ì¹˜ ì´ì „ë§Œ ë³¼ ìˆ˜ ìˆìŒ (auto-regressive)
- **ìš©ë„**: ìƒì„± ì¤‘ì¸ ë¬¸ì¥ì˜ ì´ì „ ì»¨í…ìŠ¤íŠ¸ í™œìš©

### 3. Encoder-Decoder Attention (Cross-Attention)
- **Query**: Decoderì˜ ì¶œë ¥
- **Key, Value**: Encoderì˜ ì¶œë ¥
- **ìš©ë„**: ì†ŒìŠ¤ ë¬¸ì¥ê³¼ íƒ€ê²Ÿ ë¬¸ì¥ ê°„ì˜ ê´€ê³„ íŒŒì•…

## ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë…¼ë¬¸ ê¸°ë³¸ ì„¤ì •)

| íŒŒë¼ë¯¸í„° | ê°’ | ì„¤ëª… |
|---------|---|------|
| d_model | 512 | ëª¨ë¸ ì°¨ì› |
| N (layers) | 6 | Encoder/Decoder ë ˆì´ì–´ ìˆ˜ |
| h (heads) | 8 | Multi-head attentionì˜ head ìˆ˜ |
| d_k, d_v | 64 | Key, Value ì°¨ì› (d_model/h) |
| d_ff | 2048 | Feed-forward ë‚´ë¶€ ì°¨ì› |
| dropout | 0.1 | ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ |

## ğŸš€ ëª¨ë¸ ì‚¬ìš©ë²•

```python
# ëª¨ë¸ ìƒì„± (ë…¼ë¬¸ ê¸°ë³¸ ì„¤ì •)
model = create_transformer_model(
    src_vocab_size=10000,
    tgt_vocab_size=10000,
    d_model=512,
    num_heads=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    d_ff=2048
)

# ì…ë ¥ ì¤€ë¹„
src = torch.randint(1, 1000, (batch_size, src_seq_len))  # ì†ŒìŠ¤ ë¬¸ì¥
tgt = torch.randint(1, 1000, (batch_size, tgt_seq_len))  # íƒ€ê²Ÿ ë¬¸ì¥

# Forward pass
output = model(src, tgt)  # (batch_size, tgt_seq_len, vocab_size)
```

## ğŸ² Maskì˜ ì¢…ë¥˜ì™€ ì—­í• 

### 1. Padding Mask
```python
def create_padding_mask(seq, pad_idx=0):
    mask = (seq != pad_idx).unsqueeze(1).unsqueeze(1)
    return mask
```
- **ëª©ì **: íŒ¨ë”© í† í°ì— attention ì£¼ì§€ ì•Šê¸°
- **ì ìš©**: Encoderì™€ Decoder ëª¨ë‘

### 2. Causal Mask (Look-ahead Mask)
```python
def create_causal_mask(size):
    mask = torch.triu(torch.ones(size, size), diagonal=1)
    return (mask == 0).unsqueeze(0).unsqueeze(0)
```
- **ëª©ì **: ë¯¸ë˜ í† í° ì°¸ì¡° ë°©ì§€
- **ì ìš©**: Decoderì˜ self-attentionë§Œ

## ğŸ”„ Trainingê³¼ Inference

### Training
- **Teacher Forcing**: ì‹¤ì œ íƒ€ê²Ÿ ì‹œí€€ìŠ¤ë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
- **Loss**: Cross-entropy loss
- **Optimizer**: Adam with learning rate scheduling

### Inference
- **Auto-regressive**: í•œ ë²ˆì— í•˜ë‚˜ì”© í† í° ìƒì„±
- **Beam Search**: ì—¬ëŸ¬ í›„ë³´ ì¤‘ ìµœì  ì„ íƒ

## ğŸŒŸ ë…¼ë¬¸ì˜ í˜ì‹ ì„±

1. **RNN/CNN ì œê±°**: ìˆœì „íˆ attentionë§Œìœ¼ë¡œ êµ¬ì„±
2. **ë³‘ë ¬í™” ê°€ëŠ¥**: RNNì²˜ëŸ¼ ìˆœì°¨ì ì´ì§€ ì•Šì•„ í›ˆë ¨ ì†ë„ í–¥ìƒ
3. **Long-range Dependencies**: ê±°ë¦¬ì— ê´€ê³„ì—†ì´ ì§ì ‘ì ì¸ ì—°ê²°
4. **ë²”ìš©ì„±**: ë‹¤ì–‘í•œ sequence-to-sequence íƒœìŠ¤í¬ì— ì ìš© ê°€ëŠ¥

## ğŸ”— í›„ì† ì—°êµ¬ì— ë¯¸ì¹œ ì˜í–¥

- **BERT**: Encoder-only Transformer
- **GPT**: Decoder-only Transformer  
- **T5**: Text-to-Text Transfer Transformer
- **Vision Transformer**: ì´ë¯¸ì§€ ë¶„ì•¼ ì ìš©
- **Switch Transformer**: Sparse MoE ì ìš©

ì´ êµ¬í˜„ì€ ë…¼ë¬¸ì˜ ìˆ˜ì‹ê³¼ ì•„í‚¤í…ì²˜ë¥¼ ìµœëŒ€í•œ ì¶©ì‹¤í•˜ê²Œ ì¬í˜„í•˜ì—¬, Transformerì˜ í•µì‹¬ ê°œë…ì„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.